{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1 - y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, y_oh=None, derivative=False):\n",
    "    if derivative:\n",
    "        y_pred = softmax(x)\n",
    "        k = np.nonzero(y_pred * y_oh)\n",
    "        pk = y_pred[k]\n",
    "        y_pred[k] = pk * (1.0 - pk)\n",
    "        return y_pred\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificação binária\n",
    "def binary_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / (y_pred * (1 - y_pred) * y.shape[0])\n",
    "    \n",
    "    return -np.mean(y*np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    y_sigmoid = sigmoid(y_pred)\n",
    "    \n",
    "    if derivative:\n",
    "        return -(y - y_sigmoid) / y.shape[0]\n",
    "    return -np.mean(y*np.log(y_sigmoid) + (1 - y)*np.log(1 - y_sigmoid))\n",
    "\n",
    "#Classificação multiclasse\n",
    "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    k = np.nonzero(y_pred * y_oh)\n",
    "    pk = y_pred[k]\n",
    "    if derivative:\n",
    "        y_pred[k] = (-1.0 / pk)\n",
    "        return y_pred\n",
    "    return np.mean(-np.log(pk))\n",
    "\n",
    "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_softmax = softmax(y_pred)\n",
    "    if derivative:\n",
    "        return -(y_oh - y_softmax) / y_oh.shape[0]    \n",
    "    return neg_log_likelihood(y_oh, y_softmax)\n",
    "\n",
    "#Regressão\n",
    "def mae(y, y_pred,derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(y_pred > y, 1, -1) / y.shape[0]\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "def mse(y, y_pred,derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / y.shape[0]\n",
    "    return 0.5 * np.mean((y - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(rows, cols):\n",
    "    return np.zeros((rows, cols))\n",
    "\n",
    "def ones(rows, cols):\n",
    "    return np.ones((rows, cols))\n",
    "\n",
    "def random_normal(rows, cols):\n",
    "    return np.random.randn(rows, cols)\n",
    "\n",
    "def random_uniform(rows, cols):\n",
    "    return np.random.rand(rows, cols)\n",
    "\n",
    "def glorot_normal(rows, cols):\n",
    "    std_dev = np.sqrt(2.0 / (rows + cols))\n",
    "    return std_dev * np.random.randn(rows, cols)\n",
    "\n",
    "def glorot_uniform(rows, cols):\n",
    "    limit = np.sqrt(6.0 / (rows + cols))\n",
    "    return limit * np.random.randn(rows, cols) - limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularização\n",
    "def l1_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        weights = [np.where(w < 0, -1, w) for w in weights]\n",
    "        return np.array([np.where(w > 0, 1, w) for w in weights])\n",
    "    return np.sum([np.sum(np.abs*(w)) for w in weights])\n",
    "\n",
    "def l2_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        return weights\n",
    "    return 0.5 * np.sum(weights**2)\n",
    "\n",
    "#Learning rate decay\n",
    "def none_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate\n",
    "\n",
    "def time_based_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return 1.0 / (1 + decay_rate  * epoch)\n",
    "\n",
    "def exponential_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** epoch\n",
    "\n",
    "def staircase_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "#Batch generator\n",
    "def batch_sequential(x, y, batch_size=None):\n",
    "    batch_size = x.shape[0] if batch_size is None else batch_size\n",
    "    n_batches = x.shape[0] // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        offset = batch_size * batch\n",
    "        x_batch, y_batch = x[offset:offset+batch_size], y[offset:offset+batch_size]\n",
    "        yield (x_batch, y_batch)\n",
    "        \n",
    "def batch_shuffle(x, y, batch_size=None):\n",
    "    shuffle_index = np.random.permutation(range(x.shape[0]))\n",
    "    return batch_sequential(x[shuffle_index], y[shuffle_index], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_dim, output_dim, weights_initializer=random_normal, biases_initializer=ones,activation=linear, droput_prob=0, reg_func=l2_regularization, reg_strength=0.0, batch_norm=False, bn_decay=0.9, is_trainable=True):\n",
    "        self.input_dim = None\n",
    "        self.weights = weights_initializer(output_dim, input_dim)\n",
    "        self.biases = biases_initializer(1, output_dim)\n",
    "        self.activation = activation\n",
    "        self.droput_prob = droput_prob\n",
    "        self.reg_func = reg_func\n",
    "        self.reg_strength = reg_strength\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn_decay = bn_decay\n",
    "        self.is_trainable = is_trainable\n",
    "        self.gamma, self.beta = ones(1, output_dim), zeros(1, output_dim)\n",
    "\n",
    "        self._activ_inp, self._activ_out = None, None\n",
    "        self._dweights, self._dbiases, self._prev_dweights = None, None, 0.0\n",
    "        self._dropout_mask = None\n",
    "        self._dgamma, self._dbeta = None,None\n",
    "        self._pop_mean, self._pop_var = zeros(1, output_dim), zeros(1, output_dim)\n",
    "        self._bn_cache = None\n",
    "    \n",
    "class NeuronNetwork():\n",
    "    def __init__(self, cost_func=mse, learning_rate=1e-3,lr_decay_method=none_decay, lr_decay_rate=0.0, lr_decay_steps=1,momentum=0.0, patience=np.inf):\n",
    "        self.layers = []\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = self.lr_initial = learning_rate\n",
    "        self.lr_decay_method = lr_decay_method\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.momentum = momentum\n",
    "        self.patience, self.waiting = patience, 0\n",
    "        self._best_model, self._best_loss = self.layers, np.inf\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None, epochs=100, verbose=10, batch_gen=batch_sequential, batch_size=None, batch_norm=False, bn_decay=0.9):\n",
    "        x_val, y_val = (x_train, y_train) if (x_val is None or y_val is None) else (x_val, y_val)\n",
    "        \n",
    "        loss_val2 = []\n",
    "        loss_train2 = []\n",
    "        \n",
    "        for epoch in range(epochs+1):\n",
    "            self.learning_rate = self.lr_decay_method(self.lr_initial, epoch, self.lr_decay_rate, self.lr_decay_steps)\n",
    "            \n",
    "            for x_batch, y_batch in batch_gen(x_train, y_train, batch_size):\n",
    "                y_pred = self.__feedfoward(x_batch)\n",
    "                self.__backprop(y_batch, y_pred)\n",
    "            \n",
    "            loss_val = self.cost_func(y_val, self.predict(x_val))\n",
    "            loss_val2.append(loss_val)\n",
    "            loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "            loss_train2.append(loss_train)\n",
    "            \n",
    "            if loss_val < self._best_loss:\n",
    "                self._best_model, self._best_loss = self.layers, loss_val\n",
    "                self.waiting = 0\n",
    "            else:\n",
    "                self.waiting += 1\n",
    "                if self.waiting >= self.patience:\n",
    "                    self.layers = self._best_model\n",
    "                    return loss_val2, loss_train2\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "                loss_train2.append(loss_train)\n",
    "                loss_reg = (1.0 / y_train.shape[0]) * np.sum([layer.reg_func(layer.weights) * layer.reg_strength for layer in self.layers])\n",
    "                print(\"Epoch {0:=4}/{1} loss_train: {2:.8f} + {3:.8f} = {4:.8f} loss_val = {5:.8f}\".format(epoch, epochs,loss_train, loss_reg,loss_train+loss_reg, loss_val))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.__feedfoward(x, is_training=False)\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        pkl.dump(self, open(file_path, 'wb'), -1)\n",
    "        \n",
    "    def load(file_path):\n",
    "        return pkl.load(open(file_path, 'rb'))\n",
    "    \n",
    "    def __feedfoward(self, x, is_training=True):\n",
    "        self.layers[0].input = x\n",
    "        for current_layer , next_layer in zip(self.layers, self.layers[1:] + [Layer(0,0)]):\n",
    "            y = np.dot(current_layer.input, current_layer.weights.T) + current_layer.biases\n",
    "            y = batchnorm_foward(current_layer, y, is_training) if current_layer.batch_norm else y\n",
    "            current_layer.droput_mask = np.random.binomial(1, 1.0 - current_layer.droput_prob, y.shape) / (1.0 - current_layer.droput_prob)\n",
    "            current_layer._activ_inp = y\n",
    "            current_layer._activ_out = current_layer.activation(y) * (current_layer.droput_mask if is_training else 1.0)\n",
    "            next_layer.input = current_layer._activ_out\n",
    "        return self.layers[-1]._activ_out\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        for layer in reversed(self.layers):\n",
    "            deactivation = layer.activation(layer._activ_inp, derivative=True) * last_delta * layer.droput_mask\n",
    "            deactivation = batchnorm_backward(layer, deactivation) if layer.batch_norm else deactivation\n",
    "            last_delta = np.dot(deactivation, layer.weights)\n",
    "            layer._dweights = np.dot(deactivation.T, layer.input)\n",
    "            layer._dbiases = 1.0 * deactivation.sum(axis=0, keepdims=True)\n",
    "            \n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.is_trainable:\n",
    "                layer._dweights = layer._dweights + (1.0 / y.shape[0]) * layer.reg_strength * layer.reg_func(layer.weights, derivative=True)\n",
    "                layer._prev_dweights = - self.learning_rate * layer._dweights + self.momentum*layer._prev_dweights\n",
    "                layer.weights = layer.weights - self.learning_rate * layer._dweights\n",
    "                layer.biases = layer.biases - self.learning_rate * layer._dbiases\n",
    "\n",
    "                if layer.batch_norm:\n",
    "                    layer.gamma = layer.gamma - self.learning_rate * layer._dgamma\n",
    "                    layer.beta = layer.beta - self.learning_rate * layer._dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand():\n",
    "    return round(randint(0,1) + uniform(-0.1, 0.1),3)\n",
    "\n",
    "def binaryToDecimal(n): \n",
    "    return int(n,2) \n",
    "\n",
    "n_instances = 1500\n",
    "X = np.zeros((n_instances, 3), dtype=np.float32)\n",
    "y = np.zeros(n_instances, dtype=np.int)\n",
    "\n",
    "\n",
    "for i in range(n_instances):\n",
    "    val = np.array([rand(), rand(), rand()])\n",
    "    X[i] = val\n",
    "\n",
    "    code = np.array([round(val[0]), round(val[1]), round(val[2])])\n",
    "    label = np.array([code[0]*4 + code[1]*2 + code[2]])\n",
    "    y[i] = label\n",
    "    \n",
    "oh = OneHotEncoder()\n",
    "y_oh = oh.fit_transform(y.reshape(-1,1)).toarray()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1],8)\n",
    "        self.fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 0.1\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500):\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer=\"glorot_uniform\", activation = 'linear', input_dim=X_train.shape[1]))\n",
    "classifier.add(Dense(8, kernel_initializer=\"glorot_uniform\", activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, batch_size = 32, epochs = 150,validation_data=(X_val, y_val),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10,min_delta=0.1, verbose=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda questão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) XOR\n",
    "a_data_X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype='float32')\n",
    "a_data_y = np.array([[0],[1],[1],[0]],dtype='float32')\n",
    "\n",
    "X_train, X_val, X_test = a_data_X, a_data_X, a_data_X\n",
    "y_train, y_val, y_test = a_data_y, a_data_y, a_data_y\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_val = torch.from_numpy(X_val)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 2] at entry 0 and [4, 1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-a56dae1742cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Forward pass: compute predicted y by passing x to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 2] at entry 0 and [4, 1] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(4, 1),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if t % 100 == 99:\n",
    "            print(t, loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset=[X_val, y_val], batch_size=32)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=[X_train, y_train], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [4, 2] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-480c422acd61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [4, 2] at entry 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred, output_dict=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
