{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import *\n",
    "from utils import plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1 - y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, y_oh=None, derivative=False):\n",
    "    if derivative:\n",
    "        y_pred = softmax(x)\n",
    "        k = np.nonzero(y_pred * y_oh)\n",
    "        pk = y_pred[k]\n",
    "        y_pred[k] = pk * (1.0 - pk)\n",
    "        return y_pred\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificação binária\n",
    "def binary_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / (y_pred * (1 - y_pred) * y.shape[0])\n",
    "    \n",
    "    return -np.mean(y*np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    y_sigmoid = sigmoid(y_pred)\n",
    "    \n",
    "    if derivative:\n",
    "        return -(y - y_sigmoid) / y.shape[0]\n",
    "    return -np.mean(y*np.log(y_sigmoid) + (1 - y)*np.log(1 - y_sigmoid))\n",
    "\n",
    "#Classificação multiclasse\n",
    "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    k = np.nonzero(y_pred * y_oh)\n",
    "    pk = y_pred[k]\n",
    "    if derivative:\n",
    "        y_pred[k] = (-1.0 / pk)\n",
    "        return y_pred\n",
    "    return np.mean(-np.log(pk))\n",
    "\n",
    "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_softmax = softmax(y_pred)\n",
    "    if derivative:\n",
    "        return -(y_oh - y_softmax) / y_oh.shape[0]    \n",
    "    return neg_log_likelihood(y_oh, y_softmax)\n",
    "\n",
    "#Regressão\n",
    "def mae(y, y_pred,derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(y_pred > y, 1, -1) / y.shape[0]\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "def mse(y, y_pred,derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / y.shape[0]\n",
    "    return 0.5 * np.mean((y - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(rows, cols):\n",
    "    return np.zeros((rows, cols))\n",
    "\n",
    "def ones(rows, cols):\n",
    "    return np.ones((rows, cols))\n",
    "\n",
    "def random_normal(rows, cols):\n",
    "    return np.random.randn(rows, cols)\n",
    "\n",
    "def random_uniform(rows, cols):\n",
    "    return np.random.rand(rows, cols)\n",
    "\n",
    "def glorot_normal(rows, cols):\n",
    "    std_dev = np.sqrt(2.0 / (rows + cols))\n",
    "    return std_dev * np.random.randn(rows, cols)\n",
    "\n",
    "def glorot_uniform(rows, cols):\n",
    "    limit = np.sqrt(6.0 / (rows + cols))\n",
    "    return limit * np.random.randn(rows, cols) - limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularização\n",
    "def l1_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        weights = [np.where(w < 0, -1, w) for w in weights]\n",
    "        return np.array([np.where(w > 0, 1, w) for w in weights])\n",
    "    return np.sum([np.sum(np.abs*(w)) for w in weights])\n",
    "\n",
    "def l2_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        return weights\n",
    "    return 0.5 * np.sum(weights**2)\n",
    "\n",
    "#Learning rate decay\n",
    "def none_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate\n",
    "\n",
    "def time_based_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return 1.0 / (1 + decay_rate  * epoch)\n",
    "\n",
    "def exponential_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** epoch\n",
    "\n",
    "def staircase_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** (epoch // decay_steps)\n",
    "\n",
    "#Batch generator\n",
    "def batch_sequential(x, y, batch_size=None):\n",
    "    batch_size = x.shape[0] if batch_size is None else batch_size\n",
    "    n_batches = x.shape[0] // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        offset = batch_size * batch\n",
    "        x_batch, y_batch = x[offset:offset+batch_size], y[offset:offset+batch_size]\n",
    "        yield (x_batch, y_batch)\n",
    "        \n",
    "def batch_shuffle(x, y, batch_size=None):\n",
    "    shuffle_index = np.random.permutation(range(x.shape[0]))\n",
    "    return batch_sequential(x[shuffle_index], y[shuffle_index], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_dim, output_dim, weights_initializer=random_normal, biases_initializer=ones,activation=linear, droput_prob=0, reg_func=l2_regularization, reg_strength=0.0, batch_norm=False, bn_decay=0.9, is_trainable=True):\n",
    "        self.input_dim = None\n",
    "        self.weights = weights_initializer(output_dim, input_dim)\n",
    "        self.biases = biases_initializer(1, output_dim)\n",
    "        self.activation = activation\n",
    "        self.droput_prob = droput_prob\n",
    "        self.reg_func = reg_func\n",
    "        self.reg_strength = reg_strength\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn_decay = bn_decay\n",
    "        self.is_trainable = is_trainable\n",
    "        self.gamma, self.beta = ones(1, output_dim), zeros(1, output_dim)\n",
    "\n",
    "        self._activ_inp, self._activ_out = None, None\n",
    "        self._dweights, self._dbiases, self._prev_dweights = None, None, 0.0\n",
    "        self._dropout_mask = None\n",
    "        self._dgamma, self._dbeta = None,None\n",
    "        self._pop_mean, self._pop_var = zeros(1, output_dim), zeros(1, output_dim)\n",
    "        self._bn_cache = None\n",
    "    \n",
    "class NeuronNetwork():\n",
    "    def __init__(self, cost_func=mse, learning_rate=1e-3,lr_decay_method=none_decay, lr_decay_rate=0.0, lr_decay_steps=1,momentum=0.0, patience=np.inf):\n",
    "        self.layers = []\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = self.lr_initial = learning_rate\n",
    "        self.lr_decay_method = lr_decay_method\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.momentum = momentum\n",
    "        self.patience, self.waiting = patience, 0\n",
    "        self._best_model, self._best_loss = self.layers, np.inf\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None, epochs=100, verbose=10, batch_gen=batch_sequential, batch_size=None, batch_norm=False, bn_decay=0.9):\n",
    "        x_val, y_val = (x_train, y_train) if (x_val is None or y_val is None) else (x_val, y_val)\n",
    "        \n",
    "        loss_val2 = []\n",
    "        loss_train2 = []\n",
    "        \n",
    "        for epoch in range(epochs+1):\n",
    "            self.learning_rate = self.lr_decay_method(self.lr_initial, epoch, self.lr_decay_rate, self.lr_decay_steps)\n",
    "            \n",
    "            for x_batch, y_batch in batch_gen(x_train, y_train, batch_size):\n",
    "                y_pred = self.__feedfoward(x_batch)\n",
    "                self.__backprop(y_batch, y_pred)\n",
    "            \n",
    "            loss_val = self.cost_func(y_val, self.predict(x_val))\n",
    "            loss_val2.append(loss_val)\n",
    "            loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "            loss_train2.append(loss_train)\n",
    "            \n",
    "            if loss_val < self._best_loss:\n",
    "                self._best_model, self._best_loss = self.layers, loss_val\n",
    "                self.waiting = 0\n",
    "            else:\n",
    "                self.waiting += 1\n",
    "                if self.waiting >= self.patience:\n",
    "                    self.layers = self._best_model\n",
    "                    return loss_val2, loss_train2\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "                loss_train2.append(loss_train)\n",
    "                loss_reg = (1.0 / y_train.shape[0]) * np.sum([layer.reg_func(layer.weights) * layer.reg_strength for layer in self.layers])\n",
    "                print(\"Epoch {0:=4}/{1} loss_train: {2:.8f} + {3:.8f} = {4:.8f} loss_val = {5:.8f}\".format(epoch, epochs,loss_train, loss_reg,loss_train+loss_reg, loss_val))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.__feedfoward(x, is_training=False)\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        pkl.dump(self, open(file_path, 'wb'), -1)\n",
    "        \n",
    "    def load(file_path):\n",
    "        return pkl.load(open(file_path, 'rb'))\n",
    "    \n",
    "    def __feedfoward(self, x, is_training=True):\n",
    "        self.layers[0].input = x\n",
    "        for current_layer , next_layer in zip(self.layers, self.layers[1:] + [Layer(0,0)]):\n",
    "            y = np.dot(current_layer.input, current_layer.weights.T) + current_layer.biases\n",
    "            y = batchnorm_foward(current_layer, y, is_training) if current_layer.batch_norm else y\n",
    "            current_layer.droput_mask = np.random.binomial(1, 1.0 - current_layer.droput_prob, y.shape) / (1.0 - current_layer.droput_prob)\n",
    "            current_layer._activ_inp = y\n",
    "            current_layer._activ_out = current_layer.activation(y) * (current_layer.droput_mask if is_training else 1.0)\n",
    "            next_layer.input = current_layer._activ_out\n",
    "        return self.layers[-1]._activ_out\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        for layer in reversed(self.layers):\n",
    "            deactivation = layer.activation(layer._activ_inp, derivative=True) * last_delta * layer.droput_mask\n",
    "            deactivation = batchnorm_backward(layer, deactivation) if layer.batch_norm else deactivation\n",
    "            last_delta = np.dot(deactivation, layer.weights)\n",
    "            layer._dweights = np.dot(deactivation.T, layer.input)\n",
    "            layer._dbiases = 1.0 * deactivation.sum(axis=0, keepdims=True)\n",
    "            \n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.is_trainable:\n",
    "                layer._dweights = layer._dweights + (1.0 / y.shape[0]) * layer.reg_strength * layer.reg_func(layer.weights, derivative=True)\n",
    "                layer._prev_dweights = - self.learning_rate * layer._dweights + self.momentum*layer._prev_dweights\n",
    "                layer.weights = layer.weights - self.learning_rate * layer._dweights\n",
    "                layer.biases = layer.biases - self.learning_rate * layer._dbiases\n",
    "\n",
    "                if layer.batch_norm:\n",
    "                    layer.gamma = layer.gamma - self.learning_rate * layer._dgamma\n",
    "                    layer.beta = layer.beta - self.learning_rate * layer._dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand():\n",
    "    return round(randint(0,1) + uniform(-0.1, 0.1),3)\n",
    "\n",
    "def binaryToDecimal(n): \n",
    "    return int(n,2) \n",
    "\n",
    "n_instances = 1500\n",
    "X = np.zeros((n_instances, 3), dtype=np.float32)\n",
    "y = np.zeros(n_instances, dtype=np.int)\n",
    "\n",
    "\n",
    "for i in range(n_instances):\n",
    "    val = np.array([rand(), rand(), rand()])\n",
    "    X[i] = val\n",
    "\n",
    "    code = np.array([round(val[0]), round(val[1]), round(val[2])])\n",
    "    label = np.array([code[0]*4 + code[1]*2 + code[2]])\n",
    "    y[i] = label\n",
    "    \n",
    "oh = OneHotEncoder()\n",
    "y_oh = oh.fit_transform(y.reshape(-1,1)).toarray()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1],8)\n",
    "        self.fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 0.1\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500):\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer=\"glorot_uniform\", activation = 'linear', input_dim=X_train.shape[1]))\n",
    "classifier.add(Dense(8, kernel_initializer=\"glorot_uniform\", activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, batch_size = 32, epochs = 150,validation_data=(X_val, y_val),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10,min_delta=0.1, verbose=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda questão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) XOR\n",
    "a_data_X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype='float32')\n",
    "a_data_y = np.array([[0],[1],[1],[0]],dtype='float32')\n",
    "\n",
    "X_train, X_val, X_test = a_data_X, a_data_X, a_data_X\n",
    "y_train, y_val, y_test = a_data_y, a_data_y, a_data_y\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_val = torch.from_numpy(X_val)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7c7b13dfa0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=list(np.array(y).ravel()), s=15, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   10/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   20/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   30/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   40/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   50/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   60/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   70/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   80/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch   90/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n",
      "Epoch  100/100 loss_train: 0.80961447 + 0.00000000 = 0.80961447 loss_val = 0.80961447\n"
     ]
    }
   ],
   "source": [
    "input_dim, output_dim = x.shape[1], y.shape[1]\n",
    "\n",
    "# insira sua rede aqui!\n",
    "nn = NeuronNetwork(cost_func=binary_cross_entropy, learning_rate=0.001)\n",
    "nn.layers.append(Layer(input_dim=2, output_dim=4, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=4, output_dim=1, activation=sigmoid))\n",
    "nn.fit(x, y, epochs=100, batch_size=32, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições:\n",
      "[[0.32252066]\n",
      " [0.35404585]\n",
      " [0.21075148]\n",
      " [0.22405771]]\n",
      "Acurácia: 50.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = nn.predict(x)\n",
    "print('Predições:', y_pred, sep='\\n')\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "\n",
    "plot.classification_predictions(x, y, is_binary=True, nn=nn, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [4, 2] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-480c422acd61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [4, 2] at entry 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred, output_dict=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
